{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0df7ec2-d656-424b-b14c-5fb359fd4020",
   "metadata": {},
   "source": [
    "# Assignment 3: Non-Linear Models and Validation Metrics (37 total marks)\n",
    "### Due: October 24 at 11:59pm\n",
    "\n",
    "### Name: SOUMINI MOHANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31b39a",
   "metadata": {},
   "source": [
    "### In this assignment, you will need to write code that uses non-linear models to perform classification and regression tasks. You will also be asked to describe the process by which you came up with the code. More details can be found below. Please cite any websites or AI tools that you used to help you with this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf275ca7",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b67a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee2d2c3",
   "metadata": {},
   "source": [
    "## Part 1: Regression (14.5 marks)\n",
    "\n",
    "For this section, we will be continuing with the concrete example from yellowbrick. You will need to compare these results to the results from the previous assignment. Please use the results from the solution if you were unable to complete Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8219f163",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (0.5 marks)\n",
    "\n",
    "The data used for this task can be downloaded using the yellowbrick library: \n",
    "https://www.scikit-yb.org/en/latest/api/datasets/concrete.html\n",
    "\n",
    "Use the yellowbrick function `load_concrete()` to load the concrete dataset into the feature matrix `X` and target vector `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af8bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import concrete dataset from yellowbrick library\n",
    "from yellowbrick.datasets import load_concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4805de5e-8182-4027-9a2c-ffdef652dbb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the concrete dataset into a feature matrix X and a target vector y\n",
    "X, y = load_concrete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c364c1-76c9-43cb-b6f7-5a0813174441",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feature matrix X:  (1030, 8)\n",
      "The type of feature matrix X:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the feature matrix X \n",
    "# Shape indicates (n_samples, n_features)\n",
    "print(\"The shape of feature matrix X: \", X.shape) \n",
    "print(\"The type of feature matrix X: \", type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c188d9ad-a126-4c1e-af16-0f345e09ed9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of target vector y:  (1030,)\n",
      "The type of taget vector y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the target vector y \n",
    "# Shape indicates length (n_samples)\n",
    "print(\"The shape of target vector y: \", y.shape) \n",
    "print(\"The type of taget vector y: \", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d461e89-b010-4caa-92b5-99cf299a74f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>splast</th>\n",
       "      <th>coarse</th>\n",
       "      <th>fine</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cement   slag  ash  water  splast  coarse   fine  age\n",
       "0   540.0    0.0  0.0  162.0     2.5  1040.0  676.0   28\n",
       "1   540.0    0.0  0.0  162.0     2.5  1055.0  676.0   28\n",
       "2   332.5  142.5  0.0  228.0     0.0   932.0  594.0  270\n",
       "3   332.5  142.5  0.0  228.0     0.0   932.0  594.0  365\n",
       "4   198.6  132.4  0.0  192.0     0.0   978.4  825.5  360"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the various columns present in the feature matrix \n",
    "# (i.e., 8 features = 8 columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7194eaf8-f6be-4bed-a894-3d868e71cafe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.986111\n",
       "1    61.887366\n",
       "2    40.269535\n",
       "3    41.052780\n",
       "4    44.296075\n",
       "5    47.029847\n",
       "6    43.698299\n",
       "7    36.447770\n",
       "8    45.854291\n",
       "9    39.289790\n",
       "Name: strength, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the 1D target array having only 1 column (i.e., strength)\n",
    "y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea4cc",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (0 marks)\n",
    "\n",
    "Data processing was completed in the previous assignment. No need to repeat here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f80b8f70-9740-4388-bc3c-a9611404d0af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cement    0\n",
       "slag      0\n",
       "ash       0\n",
       "water     0\n",
       "splast    0\n",
       "coarse    0\n",
       "fine      0\n",
       "age       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the X dataframe contains any missing or NaN values\n",
    "# sum() indicates the total count of NaN values present in each column \n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a04da29-8731-4f67-a8e1-6ca33f6cf610",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the target vector y has any missing values \n",
    "# present in its 1 column (i.e., strength)\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06dee3f5-251a-44b8-b945-1b9a87f0f56d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As seen in the above 2 commands, there are no missing or NaN values\n",
    "# in either the feature matrix X or the target vector y\n",
    "# Hence it is not necessary to use a method to fill in missing values\n",
    "# If missing values had existed, we would have used the following commands\n",
    "# assuming that we are filling it with Zeros instead of just dropping \n",
    "# the row or column containing missing or NaN values \n",
    "# X.fillna(0) # For feature matrix X\n",
    "# y.fillna(0) # For target vector y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a245d00",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import the Decision Tree, Random Forest and Gradient Boosting Machines regression models from sklearn\n",
    "2. Instantiate the three models with `max_depth = 5`. Are there any other parameters that you will need to set?\n",
    "3. Implement each machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f6f35e-6608-4751-9469-dde947f036fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the required training and testing sets from the given dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c54bc75-2231-4750-b898-38ebdcf78889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeRegressor(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the 1st Model - Decision Tree Regression Model \n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dt_model = DecisionTreeRegressor(max_depth=5, random_state=0)\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79c9160-2630-4f0f-a9a5-2129821f9428",
   "metadata": {},
   "source": [
    "For the Decision Tree Regression Model as seen above, in addition to the max_depth parameter, the random_state parameter was also specified, and that was set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b5e9427-8c76-4211-8f17-c00f85f364fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the 2nd Model - Random Forest Regression Model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_model = RandomForestRegressor(max_depth=5, n_estimators=100, random_state=0)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3577209c-08da-4b9d-ac96-ec4368535e5a",
   "metadata": {},
   "source": [
    "For the Random Forest Regression Model as seen above, in addition to the max_depth parameter, the random_state parameter was also specified, and that was set to 0. Also the n_estimators was set to a value of 100, since the higher the number of trees the better to learn the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06e71af5-8293-4767-acb3-47ee253e08c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_depth=5, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor(max_depth=5, random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Consider the 3rd Model - Gradient Boosting Regression Model\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb_model = GradientBoostingRegressor(max_depth=5, n_estimators=100, learning_rate=0.1, random_state=0)\n",
    "gb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8252c869-aa44-4346-98fc-e116dae471a2",
   "metadata": {},
   "source": [
    "For the Gradient Boosting Regression Model as seen above, in addition to the max_depth parameter, the random_state parameter was also specified, and that was set to 0. Also the learning_rate was set to a value of 0.1, since a lower learning rate can dramatically improve the performance of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f994e31",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model\n",
    "\n",
    "Calculate the average training and validation accuracy using mean squared error with cross-validation. To do this, you will need to set `scoring='neg_mean_squared_error'` in your `cross_validate` function and negate the results (multiply by -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "171580a2-ce93-4030-8699-443e8fbd834c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Average Training MSE: 47.82297374707941\n",
      "Decision Tree - Average Validation MSE: 74.04533535288454\n",
      "\n",
      "Random Forest - Average Training MSE: 30.29636272513661\n",
      "Random Forest - Average Validation MSE: 47.6147076815418\n",
      "\n",
      "Gradient Boosting - Average Training MSE: 3.694308275687585\n",
      "Gradient Boosting - Average Validation MSE: 23.546500376160946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "def calculate_average_mse(model, X_train, y_train, cv=5):\n",
    "    mse_scores = cross_validate(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n",
    "    train_mse = -mse_scores['train_score']\n",
    "    val_mse = -mse_scores['test_score']\n",
    "    \n",
    "    # Calculate the mean MSE scores\n",
    "    avg_train_mse = np.mean(train_mse)\n",
    "    avg_val_mse = np.mean(val_mse)\n",
    "    \n",
    "    return avg_train_mse, avg_val_mse\n",
    "\n",
    "# Calculate average MSE for Decision Tree\n",
    "avg_train_mse_dt, avg_val_mse_dt = calculate_average_mse(dt_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Calculate average MSE for Random Forest\n",
    "avg_train_mse_rf, avg_val_mse_rf = calculate_average_mse(rf_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Calculate average MSE for Gradient Boosting Machines\n",
    "avg_train_mse_gb, avg_val_mse_gb = calculate_average_mse(gb_model, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Decision Tree - Average Training MSE:\", avg_train_mse_dt)\n",
    "print(\"Decision Tree - Average Validation MSE:\", avg_val_mse_dt)\n",
    "print(\"\\nRandom Forest - Average Training MSE:\", avg_train_mse_rf)\n",
    "print(\"Random Forest - Average Validation MSE:\", avg_val_mse_rf)\n",
    "print(\"\\nGradient Boosting - Average Training MSE:\", avg_train_mse_gb)\n",
    "print(\"Gradient Boosting - Average Validation MSE:\", avg_val_mse_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc3f7a8",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy, and index: DT, RF and GB\n",
    "2. Add the accuracy results to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdc93a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa0095a4-8ef0-4c78-9b55-5171159f2f2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training accuracy (MSE) Validation accuracy (MSE)\n",
      "DT               47.822974                 74.045335\n",
      "RF               30.296363                 47.614708\n",
      "GB                3.694308                   23.5465\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame\n",
    "results = pd.DataFrame(index=[\"DT\", \"RF\", \"GB\"], columns=[\"Training accuracy (MSE)\", \"Validation accuracy (MSE)\"])\n",
    "\n",
    "# Define the models\n",
    "models = [dt_model, rf_model, gb_model]\n",
    "\n",
    "# Iterate through the models and calculate accuracy\n",
    "for idx, model in enumerate(models):\n",
    "    avg_train_mse, avg_val_mse = calculate_average_mse(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    results.loc[results.index[idx], \"Training accuracy (MSE)\"] = avg_train_mse\n",
    "    results.loc[results.index[idx], \"Validation accuracy (MSE)\"] = avg_val_mse\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd63844-14f4-4fa7-b011-804ca04d0985",
   "metadata": {},
   "source": [
    "Repeat the step above to print the R2 score instead of the mean-squared error. For this case, you can use `scoring='r2'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df8cfdd4-5ffd-4d4d-b6ba-64c6efa84ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree - Average Training R2-score: 0.8304365153519161\n",
      "Decision Tree - Average Validation R2-score: 0.7351844581475954\n",
      "\n",
      "Random Forest - Average Training R2-score: 0.8926341961695009\n",
      "Random Forest - Average Validation R2-score: 0.830004245718197\n",
      "\n",
      "Gradient Boosting - Average Training R2-score: 0.9869031337469079\n",
      "Gradient Boosting - Average Validation R2-score: 0.9161546953448305\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import r2_score as r2\n",
    "def calculate_average_r2(model, X_train, y_train, cv=5):\n",
    "    r2_scores = cross_validate(model, X_train, y_train, cv=5, scoring='r2', return_train_score=True)\n",
    "    train_r2 = r2_scores['train_score']\n",
    "    val_r2 = r2_scores['test_score']\n",
    "    \n",
    "    # Calculate the mean MSE scores\n",
    "    avg_train_r2 = np.mean(train_r2)\n",
    "    avg_val_r2 = np.mean(val_r2)\n",
    "    \n",
    "    return avg_train_r2, avg_val_r2\n",
    "\n",
    "# Calculate average MSE for Decision Tree\n",
    "avg_train_r2_dt, avg_val_r2_dt = calculate_average_r2(dt_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Calculate average MSE for Random Forest\n",
    "avg_train_r2_rf, avg_val_r2_rf = calculate_average_r2(rf_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Calculate average MSE for Gradient Boosting Machines\n",
    "avg_train_r2_gb, avg_val_r2_gb = calculate_average_r2(gb_model, X_train, y_train, cv=5)\n",
    "\n",
    "print(\"Decision Tree - Average Training R2-score:\", avg_train_r2_dt)\n",
    "print(\"Decision Tree - Average Validation R2-score:\", avg_val_r2_dt)\n",
    "print(\"\\nRandom Forest - Average Training R2-score:\", avg_train_r2_rf)\n",
    "print(\"Random Forest - Average Validation R2-score:\", avg_val_r2_rf)\n",
    "print(\"\\nGradient Boosting - Average Training R2-score:\", avg_train_r2_gb)\n",
    "print(\"Gradient Boosting - Average Validation R2-score:\", avg_val_r2_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "143fc008-2a7a-49d1-945f-ccb531401e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Training accuracy (R2) Validation accuracy (R2)\n",
      "DT               0.830437                 0.735184\n",
      "RF               0.892634                 0.830004\n",
      "GB               0.986903                 0.916155\n"
     ]
    }
   ],
   "source": [
    "# Create an empty DataFrame\n",
    "results = pd.DataFrame(index=[\"DT\", \"RF\", \"GB\"], columns=[\"Training accuracy (R2)\", \"Validation accuracy (R2)\"])\n",
    "\n",
    "# Define the models\n",
    "models = [dt_model, rf_model, gb_model]\n",
    "\n",
    "# Iterate through the models and calculate accuracy\n",
    "for idx, model in enumerate(models):\n",
    "    avg_train_r2, avg_val_r2 = calculate_average_r2(model, X_train, y_train, cv=5)\n",
    "    \n",
    "    # Store the results in the DataFrame\n",
    "    results.loc[results.index[idx], \"Training accuracy (R2)\"] = avg_train_r2\n",
    "    results.loc[results.index[idx], \"Validation accuracy (R2)\"] = avg_val_r2\n",
    "\n",
    "# Print the results DataFrame\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5257a98",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do these results compare to the results using a linear model in the previous assignment? Use values.\n",
    "1. Out of the models you tested, which model would you select for this dataset and why?\n",
    "1. If you wanted to increase the accuracy of the tree-based models, what would you do? Provide two suggestions.\n",
    "\n",
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4734fe3b-ec0c-494e-b8c6-20c30c672379",
   "metadata": {},
   "source": [
    "Answer 01:\n",
    "The validation score using MSE evaluation metric for a linear model like the linear regression model (as used in Assignment 02) was 95.9, whereas the R2-score for the same model was 0.62.\n",
    "\n",
    "Lower MSE values indicate better performance, since it indicates that the model's predictions are closer to the actual values. All three models namely, Decision Tree Regression model, Random Forest Regression model, and Gradient Boosting Regression model have lower MSE values i.e., 74, 47.6, and 23.5 respectively when compared to the MSE score of 95.9 in the linear model used in Assignment 02. \n",
    "\n",
    "Higher R2 values (closer to 1) indicate better model fit, since it suggests that a larger proportion of the variance is explained by the model. All three models namely, Decision Tree Regression model, Random Forest Regression model, and Gradient Boosting Regression model have higher R2-scores i.e., 0.73, 0.83, and 0.91 respectively when compared to the score of 0.62 in the linear model used in Assignment 02. \n",
    "\n",
    "Hence, all these three tree models perform significantly better than the linear model used in Assignment 02 for the concrete dataset. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "82bfb5b1-1645-4104-94f3-d2f39ea61cf2",
   "metadata": {},
   "source": [
    "Answer 02: \n",
    "Out of the three model, I would choose the Gradient Boosting Regression model. It has the lowest MSE (23.54) and the highest R2 score (0.91). This indicates that it provides the best overall performance in terms of both prediction accuracy (MSE) and the proportion of variance explained (R2). Lower prediction accuracy means that the model's prediction are closer to the actual values, thereby minimizing prediction errors. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a6d91c2c-0fd6-41c1-a900-487fb954e913",
   "metadata": {},
   "source": [
    "Answer 03:\n",
    "Decision Tree Regression model:\n",
    "Increasing the max_depth to 7, improves both the MSE value and the R2 score. For this model, with a max_depth of 7, the MSE value reduces from 74.04 to 54.23, and the R2 score improves from 0.73 to 0.80 indicating improvement in the model performance. \n",
    "\n",
    "Random Forest Regression model:\n",
    "Increasing the max_depth to 7, improves both the MSE value and the R2 score. For this model, with a max_depth of 7, the MSE value reduces from 47.61 to 34.93, and the R2 score improves from 0.83 to 0.87 indicating improvement in the model performance.\n",
    "\n",
    "Gradient Boosting Regression model:\n",
    "Changing the learning_rate to 0.3, improves both the MSE value and the R2 score. For this model, with a learning_rate of 0.3, the MSE value reduces from 23.54 to 21.62, and the R2 score improves from 0.91 to 0.92 indicating improvement in the model performance.\n",
    "\n",
    "Overall, increasing the max depth, increasing the number of trees (n_estimators), adjusting the learning rate, can improve the model's accuracy significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b238f4",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf494e92-8c1b-4105-940a-5298c2f2d751",
   "metadata": {},
   "source": [
    "Answer:\n",
    "I followed the steps as instructed in this notebook. \n",
    "\n",
    "The following resources were referred to for completing this question - lecture notes, Jupyter notebooks on machine learning, model selection, regression models, decision trees, Python Data Science handbook. \n",
    "\n",
    "I googled the difference between a Tree Classifier model instead of the corresponding Regression model. And how using a Decision Tree Classifier model in this exercise generates an error. \n",
    "\n",
    "I did not face any challenges since Assignment 02 was a good starting point for gaining hands-on experience on working with different models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c6de86",
   "metadata": {},
   "source": [
    "## Part 2: Classification (17.5 marks)\n",
    "\n",
    "You have been asked to develop code that can help the user classify different wine samples. Following the machine learning workflow described in class, write the relevant code in each of the steps below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d33a8",
   "metadata": {},
   "source": [
    "### Step 1: Data Input (2 marks)\n",
    "\n",
    "The data used for this task can be downloaded from UCI: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Use the pandas library to load the dataset. You must define the column headers if they are not included in the dataset \n",
    "\n",
    "You will need to split the dataset into feature matrix `X` and target vector `y`. Which column represents the target vector?\n",
    "\n",
    "Print the size and type of `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33583c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0        1    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1        1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2        1    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3        1    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4        1    13.24        2.59  2.87          21.0        118           2.80   \n",
       "..     ...      ...         ...   ...           ...        ...            ...   \n",
       "173      3    13.71        5.65  2.45          20.5         95           1.68   \n",
       "174      3    13.40        3.91  2.48          23.0        102           1.80   \n",
       "175      3    13.27        4.28  2.26          20.0        120           1.59   \n",
       "176      3    13.17        2.59  2.37          20.0        120           1.65   \n",
       "177      3    14.13        4.10  2.74          24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     OD280/OD315_of_diluted_wines  Proline  \n",
       "0                            3.92     1065  \n",
       "1                            3.40     1050  \n",
       "2                            3.17     1185  \n",
       "3                            3.45     1480  \n",
       "4                            2.93      735  \n",
       "..                            ...      ...  \n",
       "173                          1.74      740  \n",
       "174                          1.56      750  \n",
       "175                          1.56      835  \n",
       "176                          1.62      840  \n",
       "177                          1.60      560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import wine dataset\n",
    "# Define column headers based on the dataset description\n",
    "column_headers = [\n",
    "    'class', 'Alcohol', 'Malic_Acid', 'Ash', 'Ash_Alcanity', 'Magnesium',\n",
    "    'Total_Phenols', 'Flavanoids', 'Nonflavanoid_Phenols',\n",
    "    'Proanthocyanins', 'Color_Intensity', 'Hue', 'OD280/OD315_of_diluted_wines',\n",
    "    'Proline']\n",
    "# Download the dataset from the UCI repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "# Load the dataset into a pandas DataFrame with column headers\n",
    "wine_df = pd.read_csv(url, names=column_headers)\n",
    "# Display the wine dataset showing all the stated columns\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb6dd1a-50d7-41f0-8e81-03708636afb1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into X (feature matrix) and y (target vector)\n",
    "X = wine_df.drop('class', axis=1)  # Drop the 'class' column to get the feature matrix (without target vector)\n",
    "y = wine_df['class']  # 'class' column represents the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6da2b5e3-6db6-4d31-b520-95c825d0f0ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of feature matrix X:  (178, 13)\n",
      "The type of feature matrix X:  <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the feature matrix X \n",
    "# Shape indicates (n_samples, n_features)\n",
    "print(\"The shape of feature matrix X: \", X.shape) \n",
    "print(\"The type of feature matrix X: \", type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be634d8f-9330-499d-bc45-4cd56e328efc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of target vector y:  (178,)\n",
      "The type of taget vector y:  <class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "# Print the size and type of the target vector y \n",
    "# Shape indicates length (n_samples)\n",
    "print(\"The shape of target vector y: \", y.shape) \n",
    "print(\"The type of taget vector y: \", type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156db208",
   "metadata": {},
   "source": [
    "### Step 2: Data Processing (1.5 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af110",
   "metadata": {},
   "source": [
    "Print the first five rows of the dataset to inspect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea266921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4    13.24        2.59  2.87          21.0        118           2.80   \n",
       "\n",
       "   Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   OD280/OD315_of_diluted_wines  Proline  \n",
       "0                          3.92     1065  \n",
       "1                          3.40     1050  \n",
       "2                          3.17     1185  \n",
       "3                          3.45     1480  \n",
       "4                          2.93      735  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the various columns present in the feature matrix \n",
    "# Displays the first 5 rows of data\n",
    "# (i.e., 13 features = 13 columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa932bbb-5961-45a2-8fca-cbe34ae36e71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the 1D target array having only 1 column (i.e., class)\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834fc8fe",
   "metadata": {},
   "source": [
    "Check to see if there are any missing values in the dataset. If necessary, select an appropriate method to fill-in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97c6e9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alcohol                         0\n",
       "Malic_Acid                      0\n",
       "Ash                             0\n",
       "Ash_Alcanity                    0\n",
       "Magnesium                       0\n",
       "Total_Phenols                   0\n",
       "Flavanoids                      0\n",
       "Nonflavanoid_Phenols            0\n",
       "Proanthocyanins                 0\n",
       "Color_Intensity                 0\n",
       "Hue                             0\n",
       "OD280/OD315_of_diluted_wines    0\n",
       "Proline                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the X dataframe contains any missing or NaN values\n",
    "# sum() indicates the total count of NaN values present in each column \n",
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b50b1cf2-757f-42d6-aee0-5d1db022e24a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the target vector y has any missing values \n",
    "# present in its 1 column (i.e., class)\n",
    "y.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad2fd8b8-4c26-453b-9f4b-db98fff57b90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As seen in the above 2 commands, there are no missing or NaN values\n",
    "# in either the feature matrix X or the target vector y\n",
    "# Hence it is not necessary to use a method to fill in missing values\n",
    "# If missing values had existed, we would have used the following commands\n",
    "# assuming that we are filling it with Zeros instead of just dropping \n",
    "# the row or column containing missing or NaN values \n",
    "# X.fillna(0) # For feature matrix X\n",
    "# y.fillna(0) # For target vector y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f04734e-d265-4d40-87b8-0fd939284bed",
   "metadata": {},
   "source": [
    "How many samples do we have of each type of wine?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b37a6fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class\n",
      "2    71\n",
      "1    59\n",
      "3    48\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of samples for each type of wine \n",
    "wine_counts = wine_df['class'].value_counts()\n",
    "print(wine_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6c46f",
   "metadata": {},
   "source": [
    "### Step 3: Implement Machine Learning Model\n",
    "\n",
    "1. Import `SVC` and `DecisionTreeClassifier` from sklearn\n",
    "2. Instantiate models as `SVC()` and `DecisionTreeClassifier(max_depth = 3)`\n",
    "3. Implement the machine learning model with `X` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf8b6bd-fc9a-4ca4-a0bd-dd22331d9205",
   "metadata": {},
   "source": [
    "### Step 4: Validate Model \n",
    "\n",
    "Calculate the average training and validation accuracy using `cross_validate` for the two different models listed in Step 3. For this case, use `scoring='accuracy'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d67f948-f968-491d-b799-a9553bf788d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Accuracy for SVC: 0.6988821611551\n",
      "Average Validation Accuracy for SVC: 0.6628078817733991\n",
      "Average Training Accuracy for Decision Tree Classifier: 0.9929669306008384\n",
      "Average Validation Accuracy for Decision Tree Classifier: 0.9507389162561577\n"
     ]
    }
   ],
   "source": [
    "# Import Decision Tree Classifier Model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Import SVC Model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# Define the models\n",
    "svc_model = SVC()\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for SVC\n",
    "svc_scores = cross_validate(svc_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Perform cross-validation for Decision Tree Classifier\n",
    "dt_scores = cross_validate(dt_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Calculate average training and validation accuracy for each model\n",
    "svc_train_accuracy = svc_scores['train_score'].mean()\n",
    "svc_validation_accuracy = svc_scores['test_score'].mean()\n",
    "\n",
    "dt_train_accuracy = dt_scores['train_score'].mean()\n",
    "dt_validation_accuracy = dt_scores['test_score'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Average Training Accuracy for SVC:\", svc_train_accuracy)\n",
    "print(\"Average Validation Accuracy for SVC:\", svc_validation_accuracy)\n",
    "\n",
    "print(\"Average Training Accuracy for Decision Tree Classifier:\", dt_train_accuracy)\n",
    "print(\"Average Validation Accuracy for Decision Tree Classifier:\", dt_validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0bbd83",
   "metadata": {},
   "source": [
    "### Step 5: Visualize Results (4 marks)\n",
    "\n",
    "#### Step 5.1: Compare Models\n",
    "1. Create a pandas DataFrame `results` with columns: Training accuracy and Validation accuracy\n",
    "2. Add the data size, training and validation accuracy for each dataset to the `results` DataFrame\n",
    "3. Print `results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "be4b5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO: ADD YOUR CODE HERE FOR STEPS 3-5\n",
    "# Note: for any random state parameters, you can use random_state = 0\n",
    "# HINT: USING A LOOP TO STORE THE DATA IN YOUR RESULTS DATAFRAME WILL BE MORE EFFICIENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3a948aa-8695-4e03-931e-a96f2fa49119",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Data Size  Training Accuracy  Validation Accuracy\n",
      "0            SVC        178           0.698882             0.662808\n",
      "1  Decision Tree        178           0.992967             0.950739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Define the models\n",
    "svc_model = SVC()\n",
    "dt_model = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results_data = []\n",
    "\n",
    "# Perform cross-validation for SVC\n",
    "svc_scores = cross_validate(svc_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Calculate average training and validation accuracy for SVC\n",
    "svc_train_accuracy = svc_scores['train_score'].mean()\n",
    "svc_validation_accuracy = svc_scores['test_score'].mean()\n",
    "\n",
    "# Store results for SVC\n",
    "results_data.append(('SVC', X.shape[0], svc_train_accuracy, svc_validation_accuracy))\n",
    "\n",
    "# Perform cross-validation for Decision Tree Classifier\n",
    "dt_scores = cross_validate(dt_model, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Calculate average training and validation accuracy for Decision Tree Classifier\n",
    "dt_train_accuracy = dt_scores['train_score'].mean()\n",
    "dt_validation_accuracy = dt_scores['test_score'].mean()\n",
    "\n",
    "# Store results for Decision Tree Classifier\n",
    "results_data.append(('Decision Tree', X.shape[0], dt_train_accuracy, dt_validation_accuracy))\n",
    "\n",
    "# Create the results DataFrame\n",
    "results = pd.DataFrame(results_data, columns=['Model', 'Data Size', 'Training Accuracy', 'Validation Accuracy'])\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3683721-19ba-4b56-a6d7-490329e82ee6",
   "metadata": {},
   "source": [
    "#### Step 5.2: Visualize Classification Errors\n",
    "Which method gave the highest accuracy? Use this method to print the confusion matrix and classification report:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ff1b4-bb2b-499e-b386-37917637d673",
   "metadata": {
    "tags": []
   },
   "source": [
    "Answer: \n",
    "The Decision Tree Classifier model gave the highest accuracy among the two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44b091a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(max_depth=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(max_depth=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(max_depth=3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement best model\n",
    "dt_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09d21b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(170.97222222222223, 0.5, 'true value')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print confusion matrix using a heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(y_val, dt_model.predict(X_val))\n",
    "\n",
    "sns.heatmap(mat, square=True, annot=True, cbar=False)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ef95947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 1       0.93      1.00      0.97        14\n",
      "     class 2       1.00      0.94      0.97        16\n",
      "     class 3       1.00      1.00      1.00         6\n",
      "\n",
      "    accuracy                           0.97        36\n",
      "   macro avg       0.98      0.98      0.98        36\n",
      "weighted avg       0.97      0.97      0.97        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "target_names = ['class 1', 'class 2', 'class 3']\n",
    "print(classification_report(y_val, dt_model.predict(X_val), target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb00709-f714-4a20-8d59-74609758ac2e",
   "metadata": {},
   "source": [
    "### Questions (6 marks)\n",
    "1. How do the training and validation accuracy change depending on the method used? Explain with values.\n",
    "1. What are two reasons why the support vector machines model did not work as well as the tree-based model?\n",
    "1. How many samples were incorrectly classified in step 5.2? \n",
    "1. In this case, is maximizing precision or recall more important? Why?\n",
    "\n",
    "*YOUR ANSWERS HERE*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6166230a-e0c3-4d19-a83e-c56a2a688a20",
   "metadata": {},
   "source": [
    "Answer 01:\n",
    "The training and validation accuracy for the Decision Tree Classifier model was 0.99 and 0.95 respectively, while that for the SVC model was 0.69 and 0.66 respectively. This indicates that the Decision Tree Classifier model is the best suited model among the two for this dataset since it had higher accuracy values. \n",
    "\n",
    "Answer 02: \n",
    "If the data is inherently non-linear and the decision boundary is complex, SVMs might struggle to find an optimal hyperplane, leading to poorer performance. Also, SVMs have hyperparameters like the choice of kernel function (e.g., linear, polynomial, radial basis function), the regularization parameter (C), and others. The performance of an SVM can be highly sensitive to the choice of these hyperparameters. If the hyperparameters are not selected correctly through a thorough grid search or cross-validation, the SVM may underperform. \n",
    "\n",
    "SVMs are powerful models, but their performance can be affected by the linearity of the data and the need for careful hyperparameter tuning. Tree-based models, on the other hand, can handle non-linear data more naturally and are often more forgiving when it comes to hyperparameter selection. \n",
    "\n",
    "Answer 03: \n",
    "Only 1 sample was incorrectly classified in the confusion matrix.\n",
    "\n",
    "Answer 04:\n",
    "I would say precision is more important than recall in this case, since this dataset is about the classification of different types of wines. We are primarily interested in the correct identification of the wines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19728b4-0e80-45d5-a4f4-22e4dae1ab10",
   "metadata": {},
   "source": [
    "### Process Description (4 marks)\n",
    "Please describe the process you used to create your code. Cite any websites or generative AI tools used. You can use the following questions as guidance:\n",
    "1. Where did you source your code?\n",
    "1. In what order did you complete the steps?\n",
    "1. If you used generative AI, what prompts did you use? Did you need to modify the code at all? Why or why not?\n",
    "1. Did you have any challenges? If yes, what were they? If not, what helped you to be successful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016faa32-0d4e-4c49-8d13-1b8bece63afd",
   "metadata": {},
   "source": [
    "Answer:\n",
    "I followed the steps as instructed in this notebook. \n",
    "\n",
    "The following resources were referred to for completing this question - lecture notes, Jupyter notebooks on machine learning, model selection, regression models, decision trees, svc models, Python Data Science handbook. \n",
    "\n",
    "I had trouble loading the dataset, since I kept getting ValueError and issue with n_splits later on in the code. I later realized that the issue was with how the specific columns were imported. I was able to fix it after multiple tries, and it worked out fine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd7358d",
   "metadata": {},
   "source": [
    "## Part 3: Observations/Interpretation (3 marks)\n",
    "\n",
    "Describe any pattern you see in the results. Relate your findings to what we discussed during lectures. Include data to justify your findings.\n",
    "\n",
    "\n",
    "*ADD YOUR FINDINGS HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a381be99-1018-4ffe-bb6e-5f308ea2d120",
   "metadata": {},
   "source": [
    "I predicted SVM to perform better than the tree model since it can create decision boundaries with fewer features, like the 13 features used in this dataset. However the tree model seems to work better than the SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd97b6ac",
   "metadata": {},
   "source": [
    "## Part 4: Reflection (2 marks)\n",
    "Include a sentence or two about:\n",
    "- what you liked or disliked,\n",
    "- found interesting, confusing, challangeing, motivating\n",
    "while working on this assignment.\n",
    "\n",
    "\n",
    "*ADD YOUR THOUGHTS HERE*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b51f4b74-d717-472b-b7bd-033a411a75ea",
   "metadata": {},
   "source": [
    "I had trouble with the columns of the dataset later in the code, so that was a pretty interesting experience to be dealing with ValueErrors and n_splits. I found that confusing and quite challeging at the same time. This was a great example to work on for the confusion matrix as well. \n",
    "Also, I do keep getting Convergence Warning when I run the Linear SVC model part at the end of this notebook. I have not been able to figure out why the warning keeps popping up. But I have based my answer to that part of the question on the results that I obtained in addition to the warning. I would consider this to be the motivating part - it motivates me to figure out what mistake I did to get that error and make sure that it doesn't happen again. \n",
    "I did check out the documentation for it on: \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "But still haven't been able to figure out why the warning keeps popping up. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa21e53b",
   "metadata": {},
   "source": [
    "## Part 5: Bonus Question (3 marks)\n",
    "\n",
    "Repeat Part 2 and compare the support vector machines model used to `LinearSVC(max_iter=5000)`. Does using `LinearSVC` improve the results? Why or why not?\n",
    "\n",
    "Is `LinearSVC` a good fit for this dataset? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "be3baf47-4f4a-41a9-9f77-446543abbfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic_Acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Ash_Alcanity</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total_Phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid_Phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color_Intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315_of_diluted_wines</th>\n",
       "      <th>Proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>3</td>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>3</td>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>3</td>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>3</td>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>3</td>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class  Alcohol  Malic_Acid   Ash  Ash_Alcanity  Magnesium  Total_Phenols  \\\n",
       "0        1    14.23        1.71  2.43          15.6        127           2.80   \n",
       "1        1    13.20        1.78  2.14          11.2        100           2.65   \n",
       "2        1    13.16        2.36  2.67          18.6        101           2.80   \n",
       "3        1    14.37        1.95  2.50          16.8        113           3.85   \n",
       "4        1    13.24        2.59  2.87          21.0        118           2.80   \n",
       "..     ...      ...         ...   ...           ...        ...            ...   \n",
       "173      3    13.71        5.65  2.45          20.5         95           1.68   \n",
       "174      3    13.40        3.91  2.48          23.0        102           1.80   \n",
       "175      3    13.27        4.28  2.26          20.0        120           1.59   \n",
       "176      3    13.17        2.59  2.37          20.0        120           1.65   \n",
       "177      3    14.13        4.10  2.74          24.5         96           2.05   \n",
       "\n",
       "     Flavanoids  Nonflavanoid_Phenols  Proanthocyanins  Color_Intensity   Hue  \\\n",
       "0          3.06                  0.28             2.29             5.64  1.04   \n",
       "1          2.76                  0.26             1.28             4.38  1.05   \n",
       "2          3.24                  0.30             2.81             5.68  1.03   \n",
       "3          3.49                  0.24             2.18             7.80  0.86   \n",
       "4          2.69                  0.39             1.82             4.32  1.04   \n",
       "..          ...                   ...              ...              ...   ...   \n",
       "173        0.61                  0.52             1.06             7.70  0.64   \n",
       "174        0.75                  0.43             1.41             7.30  0.70   \n",
       "175        0.69                  0.43             1.35            10.20  0.59   \n",
       "176        0.68                  0.53             1.46             9.30  0.60   \n",
       "177        0.76                  0.56             1.35             9.20  0.61   \n",
       "\n",
       "     OD280/OD315_of_diluted_wines  Proline  \n",
       "0                            3.92     1065  \n",
       "1                            3.40     1050  \n",
       "2                            3.17     1185  \n",
       "3                            3.45     1480  \n",
       "4                            2.93      735  \n",
       "..                            ...      ...  \n",
       "173                          1.74      740  \n",
       "174                          1.56      750  \n",
       "175                          1.56      835  \n",
       "176                          1.62      840  \n",
       "177                          1.60      560  \n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import wine dataset\n",
    "# Define column headers based on the dataset description\n",
    "column_headers = [\n",
    "    'class', 'Alcohol', 'Malic_Acid', 'Ash', 'Ash_Alcanity', 'Magnesium',\n",
    "    'Total_Phenols', 'Flavanoids', 'Nonflavanoid_Phenols',\n",
    "    'Proanthocyanins', 'Color_Intensity', 'Hue', 'OD280/OD315_of_diluted_wines',\n",
    "    'Proline']\n",
    "# Download the dataset from the UCI repository\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\"\n",
    "# Load the dataset into a pandas DataFrame with column headers\n",
    "wine_df = pd.read_csv(url, names=column_headers)\n",
    "# Display the wine dataset showing all the stated columns\n",
    "wine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f09b4bd-76a8-42da-b41e-66f348c5f736",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into X (feature matrix) and y (target vector)\n",
    "X = wine_df.drop('class', axis=1)  # Drop the 'class' column to get the feature matrix (without target vector)\n",
    "y = wine_df['class']  # 'class' column represents the target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "30fea72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Training Accuracy for SVC: 0.8962583449774879\n",
      "Average Validation Accuracy for SVC: 0.8869458128078819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soumi\\anaconda3\\envs\\ensf-ml\\lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import the Linear SVC model from Scikit-Learn\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Create a training and testing split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Instantiate the model\n",
    "svc = LinearSVC(max_iter=5000)\n",
    "\n",
    "# Fit the model\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Perform cross-validation for SVC\n",
    "svc_scores = cross_validate(svc, X_train, y_train, cv=5, scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Calculate average training and validation accuracy for the model\n",
    "svc_training_accuracy = svc_scores['train_score'].mean()\n",
    "svc_validation_accuracy = svc_scores['test_score'].mean()\n",
    "\n",
    "# Print the results\n",
    "print(\"Average Training Accuracy for SVC:\", svc_training_accuracy)\n",
    "print(\"Average Validation Accuracy for SVC:\", svc_validation_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabc68a4",
   "metadata": {},
   "source": [
    "*ANSWER HERE*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e35a2e2-2721-4653-9dcb-45702c3c90dd",
   "metadata": {},
   "source": [
    "There is significant  improvement in the validation accuracy when compared to the SVM model used before. Linear SVC is a good fit for this dataset. I think Linear SVC models have a simpler structure and are easier to interpret compared to non-linear SVMs with complex kernel functions. Linear SVC is a good choice in this case since it is easier to interpret. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
